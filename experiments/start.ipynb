{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47120ccb6c7a4f7d985465d4c4df04b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n",
      "blocks.6.hook_resid_post\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from datasets import load_dataset  \n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['HF_TOKEN'] = \"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\"\n",
    "\n",
    "layer = 6\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", device = \"cuda\")\n",
    "\n",
    "sae, cfg_dict, _ = SAE.from_pretrained(\n",
    "    release = \"gemma-2b-res-jb\",\n",
    "    sae_id = f\"blocks.{layer}.hook_resid_post\",\n",
    "    device = \"cuda\"\n",
    ")\n",
    "\n",
    "hook_point = sae.cfg.hook_name\n",
    "print(hook_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,   714, 17489, 22352, 16125]], device='cuda:0')\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[72.5033, 70.9076, 68.8184],\n",
      "         [37.9054, 31.1921, 15.6136],\n",
      "         [65.9162, 14.2040, 13.3026],\n",
      "         [22.8027, 21.7161, 17.3864],\n",
      "         [43.6440, 12.6738, 10.7451]]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[[ 3390, 15881,  5347],\n",
      "         [ 6518, 13743,  1959],\n",
      "         [ 1571, 12529, 15173],\n",
      "         [12773, 10200, 15173],\n",
      "         [ 5192, 15173, 12030]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "sv_prompt = \" The Golden Gate Bridge\"\n",
    "sv_logits, cache = model.run_with_cache(sv_prompt, prepend_bos=True)\n",
    "tokens = model.to_tokens(sv_prompt)\n",
    "print(tokens)\n",
    "\n",
    "# get the feature activations from our SAE\n",
    "sv_feature_acts = sae.encode(cache[hook_point])\n",
    "\n",
    "# get sae_out\n",
    "sae_out = sae.decode(sv_feature_acts)\n",
    "\n",
    "# print out the top activations, focus on the indices\n",
    "print(torch.topk(sv_feature_acts, 3))\n",
    "\n",
    "# https://www.neuronpedia.org/gemma-2b/6-res-jb/5192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURE_ID = 5192 # of or relating to bridges\n",
    "steering_vector = sae.W_dec[FEATURE_ID]\n",
    "\n",
    "example_prompt = \"The new bill apportions $3 million towards\"\n",
    "\n",
    "coeff = 300\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)\n",
    "\n",
    "sae_out.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new bill apportions $3 million towards the\n",
      "The new bill apportions $3 million towards the bridge\n",
      "The new bill apportions $3 million towards the bridge,\n",
      "The new bill apportions $3 million towards the bridge, which\n",
      "The new bill apportions $3 million towards the bridge, which is\n",
      "The new bill apportions $3 million towards the bridge, which is the\n",
      "The new bill apportions $3 million towards the bridge, which is the only\n",
      "The new bill apportions $3 million towards the bridge, which is the only way\n",
      "The new bill apportions $3 million towards the bridge, which is the only way for\n",
      "The new bill apportions $3 million towards the bridge, which is the only way for the\n"
     ]
    }
   ],
   "source": [
    "# next we want to do a reconstruction test.\n",
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "def no_op_hook(mlp_out, hook):\n",
    "    return mlp_out\n",
    "\n",
    "def steering_hook(resid_pre, hook, *, steering_vector: torch.Tensor, steering_on: bool = True, coeff: float = 1.0):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return\n",
    "    \n",
    "    if steering_on:\n",
    "        resid_pre[:, :resid_pre.shape[1] - 1, :] += coeff * steering_vector\n",
    "\n",
    "\n",
    "string = \"The new bill apportions $3 million towards\" \n",
    "for i in range(10):\n",
    "    tokens = model.to_tokens(string)\n",
    "    out = model.run_with_hooks(tokens, fwd_hooks=[\n",
    "        (\n",
    "            sae.cfg.hook_name,\n",
    "            partial(\n",
    "                steering_hook, \n",
    "                steering_vector=steering_vector, \n",
    "                steering_on=True, \n",
    "                coeff=coeff\n",
    "            )\n",
    "            # partial(no_op_hook)\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    next_token = model.to_string(out[0][-1].argmax(-1).item())\n",
    "    string += next_token\n",
    "\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
